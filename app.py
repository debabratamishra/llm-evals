"""
LLM Evaluation Dashboard - Main Streamlit Application

A comprehensive dashboard for visualizing and analyzing LLM evaluation results,
including performance metrics, cost analysis, and model comparisons.
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import os
import sys
from typing import Dict, Any, List

# Add the dashboard directory to Python path
dashboard_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(dashboard_dir)

from data_loader import EvaluationDataLoader, format_model_name, calculate_efficiency_score
from visualizations import EvaluationVisualizer, MetricCalculator

# Page configuration
st.set_page_config(
    page_title="LLM Evaluation Dashboard",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_data
def load_evaluation_data(data_directory: str) -> Dict[str, Any]:
    """Load and cache evaluation data."""
    loader = EvaluationDataLoader(data_directory)
    return loader.load_all_data()

@st.cache_data
def get_summary_stats(data: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate summary statistics."""
    performance_df = data.get('performance_metrics', pd.DataFrame())
    cost_df = data.get('cost_metrics', pd.DataFrame())
    comparison_df = data.get('model_comparison', pd.DataFrame())
    
    stats = {
        'total_models': len(comparison_df['model'].unique()) if not comparison_df.empty else 0,
        'total_benchmarks': len(performance_df['benchmark'].unique()) if not performance_df.empty else 0,
        'avg_accuracy': performance_df['accuracy'].mean() if not performance_df.empty else 0,
        'avg_cost_per_1m': cost_df['cost_per_1m_tokens_usd'].mean() if not cost_df.empty else 0
    }
    
    return stats

def main():
    """Main dashboard application."""
    
    # Title and description
    st.title("üß† LLM Evaluation Dashboard")
    st.markdown("""
    **Comprehensive analysis of Large Language Model evaluation results**
    
    This dashboard provides insights into model performance, cost efficiency, and throughput metrics 
    across different benchmarks and evaluation scenarios.
    """)
    
    # Sidebar configuration
    st.sidebar.title("üìä Dashboard Controls")
    st.sidebar.markdown("---")
    
    # Data directory selection
    default_data_dir = os.path.join(dashboard_dir, "data")
    data_directory = st.sidebar.text_input(
        "Evaluation Data Directory",
        value=default_data_dir,
        help="Path to directory containing evaluation JSON files"
    )
    
    if not os.path.exists(data_directory):
        st.error(f"‚ùå Data directory not found: {data_directory}")
        st.stop()
    
    # Load data
    try:
        with st.spinner("Loading evaluation data..."):
            data = load_evaluation_data(data_directory)
        
        if not data:
            st.error("‚ùå No evaluation data found in the specified directory.")
            st.stop()
            
    except Exception as e:
        st.error(f"‚ùå Error loading data: {str(e)}")
        st.stop()
    
    # Get summary statistics
    stats = get_summary_stats(data)
    
    # Display summary metrics
    st.markdown("## üìà Overview")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="Models Evaluated",
            value=stats['total_models']
        )
        
    with col2:
        st.metric(
            label="Benchmarks",
            value=stats['total_benchmarks']
        )
        
    with col3:
        st.metric(
            label="Avg Accuracy",
            value=f"{stats['avg_accuracy']:.3f}"
        )
        
    with col4:
        st.metric(
            label="Avg Cost/1M Tokens",
            value=f"${stats['avg_cost_per_1m']:.4f}"
        )
    
    st.markdown("---")
    
    # Navigation tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "üéØ Performance & TopN Analysis", "üí∞ Cost Analysis", "üîç Model Comparison", "üìä Advanced Analytics"
    ])
    
    # Initialize visualizer
    visualizer = EvaluationVisualizer()
    
    # Performance Analysis Tab
    with tab1:
        st.header("üéØ Performance & TopN Analysis")
        
        performance_df = data.get('performance_metrics', pd.DataFrame())
        
        if not performance_df.empty:
            # Performance metric selector
            col1, col2 = st.columns([1, 3])
            
            with col1:
                available_metrics = [col for col in performance_df.columns if col.startswith(('accuracy', 'top'))]
                if available_metrics:
                    selected_metric = st.selectbox(
                        "Select Metric",
                        available_metrics,
                        index=0 if 'accuracy' in available_metrics else 0
                    )
                else:
                    selected_metric = None
                    st.warning("‚ö†Ô∏è No performance metrics available for selection.")
            
            with col2:
                st.empty()  # Spacer
            
            # Performance comparison chart
            st.write("**Performance Comparison**")
            
            if selected_metric is not None:
                fig = visualizer.create_performance_comparison(performance_df, selected_metric)
                st.plotly_chart(fig, width='stretch')
            else:
                st.info("No metric selected for performance comparison.")
            
            # TopN Analysis Section
            st.subheader("üî¢ TopN Analysis")
            
            # TopN comparison
            topn_fig = visualizer.create_topn_comparison(performance_df)
            st.plotly_chart(topn_fig, width='stretch')
            
            # Detailed TopN Analysis
            st.subheader("üìà TopN Performance Progression")
            
            model_options = ['All Models'] + list(performance_df['model'].unique())
            selected_model_topn = st.selectbox(
                "Select Model for TopN Analysis",
                model_options,
                format_func=lambda x: x if x == 'All Models' else format_model_name(x)
            )
            
            if selected_model_topn == 'All Models':
                topn_detailed_fig = visualizer.create_topn_detailed_analysis(performance_df)
            else:
                topn_detailed_fig = visualizer.create_topn_detailed_analysis(performance_df, selected_model_topn)
            
            st.plotly_chart(topn_detailed_fig, width='stretch')
            
            # Performance heatmap
            st.subheader("Performance Heatmap")
            
            heatmap_fig = visualizer.create_benchmark_heatmap(performance_df)
            st.plotly_chart(heatmap_fig, width="stretch")
            
            # Performance data table
            st.subheader("Detailed Performance Data")
            formatted_df = performance_df.copy()
            formatted_df['model'] = formatted_df['model'].apply(format_model_name)
            
            st.dataframe(formatted_df, width='stretch')
            
        else:
            st.warning("‚ö†Ô∏è No performance data available.")
    
    # Cost Analysis Tab
    with tab2:
        st.header("üí∞ Cost Analysis")
        
        cost_df = data.get('cost_metrics', pd.DataFrame())
        
        if not cost_df.empty:
            # Model selection checkboxes
            st.subheader("üîß Model Selection")
            available_models = cost_df['model'].unique()
            
            # Create checkboxes for model selection
            selected_models = []
            cols = st.columns(min(len(available_models), 3))  # Max 3 columns
            
            for i, model in enumerate(available_models):
                with cols[i % len(cols)]:
                    if st.checkbox(format_model_name(model), value=True, key=f"cost_model_{i}"):
                        selected_models.append(model)
            
            if not selected_models:
                st.warning("‚ö†Ô∏è Please select at least one model to display cost analysis.")
                return
            
            # Filter data based on selected models
            filtered_cost_df = cost_df[cost_df['model'].isin(selected_models)]
            
            # Main cost analysis chart
            st.subheader("Cost per 1M Tokens")
            
            cost_fig = visualizer.create_cost_analysis(filtered_cost_df)
            st.plotly_chart(cost_fig, width='stretch')
            
            # Cost data table
            st.subheader("Detailed Cost Data")
            formatted_cost_df = filtered_cost_df.copy()
            formatted_cost_df['model'] = formatted_cost_df['model'].apply(format_model_name)
            
            # Select only relevant columns, excluding run cost, execution time, and price per input
            columns_to_show = ['model', 'cost_per_1k_tokens_usd', 'cost_per_1m_tokens_usd', 'mode', 'source']
            # Only include columns that exist in the dataframe
            available_columns = [col for col in columns_to_show if col in formatted_cost_df.columns]
            formatted_cost_df_filtered = formatted_cost_df[available_columns]
            
            st.dataframe(formatted_cost_df_filtered, width='stretch')
            
        else:
            st.warning("‚ö†Ô∏è No cost data available.")
    
    # Model Comparison Tab
    with tab3:
        st.header("üîç Model Comparison")
        
        comparison_df = data.get('model_comparison', pd.DataFrame())
        
        if not comparison_df.empty:
            # Efficiency scatter plot
            st.subheader("Efficiency Analysis: Accuracy vs Cost")
            
            efficiency_fig = visualizer.create_efficiency_scatter(comparison_df)
            st.plotly_chart(efficiency_fig, width="stretch")
            
            # Model selector for radar chart
            st.subheader("Model Profile Analysis")
            
            model_options = comparison_df['model'].unique()
            selected_model_radar = st.selectbox(
                "Select Model for Profile Analysis",
                model_options,
                format_func=format_model_name,
                key="radar_model_selector"
            )
            
            radar_fig = visualizer.create_model_radar_chart(comparison_df, selected_model_radar)
            st.plotly_chart(radar_fig, width="stretch")
            
            # Model ranking
            st.subheader("Model Rankings")
            
            # Calculate efficiency scores
            comparison_df_copy = comparison_df.copy()
            comparison_df_copy['efficiency_score'] = comparison_df_copy.apply(
                lambda row: calculate_efficiency_score(
                    row['avg_accuracy'],
                    row['cost_per_1m_tokens']
                ), axis=1
            )
            
            # Sort by efficiency score
            ranking_df = comparison_df_copy.sort_values('efficiency_score', ascending=False)
            ranking_df['model'] = ranking_df['model'].apply(format_model_name)
            ranking_df['rank'] = range(1, len(ranking_df) + 1)
            
            # Display ranking table
            ranking_display = ranking_df[['rank', 'model', 'avg_accuracy', 'cost_per_1m_tokens', 
                                        'efficiency_score']].round(4)
            st.dataframe(ranking_display, width="stretch")
            
        else:
            st.warning("‚ö†Ô∏è No model comparison data available.")
    
    # Advanced Analytics Tab
    with tab4:
        st.header("üìä Advanced Analytics")
        
        if not data or all(df.empty for df in data.values() if isinstance(df, pd.DataFrame)):
            st.warning("‚ö†Ô∏è No data available for advanced analytics.")
        else:
            # Custom metric calculator
            st.subheader("Custom Efficiency Metrics")
            
            col1, col2 = st.columns(2)
            
            with col1:
                accuracy_weight = st.slider("Accuracy Weight", 0.0, 1.0, 0.7, 0.1)
            with col2:
                cost_weight = st.slider("Cost Weight", 0.0, 1.0, 0.3, 0.1)
            
            # Normalize weights
            total_weight = accuracy_weight + cost_weight
            if total_weight > 0:
                accuracy_weight /= total_weight
                cost_weight /= total_weight
            
            comparison_df = data.get('model_comparison', pd.DataFrame())
            
            if not comparison_df.empty:
                # Calculate custom efficiency scores
                custom_scores = []
                for _, row in comparison_df.iterrows():
                    # Normalize cost (invert so lower cost = higher score)
                    max_cost = comparison_df['cost_per_1m_tokens'].max()
                    normalized_cost = (max_cost - row['cost_per_1m_tokens']) / max_cost if max_cost > 0 else 0
                    
                    score = (row['avg_accuracy'] * accuracy_weight + 
                            normalized_cost * cost_weight)
                    custom_scores.append(score)
                
                # Create custom ranking
                custom_df = comparison_df.copy()
                custom_df['custom_efficiency'] = custom_scores
                custom_df = custom_df.sort_values('custom_efficiency', ascending=False)
                custom_df['model'] = custom_df['model'].apply(format_model_name)
                
                # Display custom ranking
                st.subheader("Custom Efficiency Ranking")
                custom_display = custom_df[['model', 'avg_accuracy', 'cost_per_1m_tokens', 
                                          'custom_efficiency']].round(4)
                st.dataframe(custom_display, width="stretch")
                
                # Visualize custom efficiency
                fig_custom = px.bar(
                    custom_df.head(10),  # Top 10 models
                    x='model',
                    y='custom_efficiency',
                    title='Custom Efficiency Scores (Top 10)',
                    labels={'model': 'Model', 'custom_efficiency': 'Custom Efficiency Score'}
                )
                fig_custom.update_layout(xaxis_tickangle=-45)
                st.plotly_chart(fig_custom, width="stretch")
            
            # Data export section
            st.subheader("üì• Data Export")
            
            export_data = st.selectbox(
                "Select data to export",
                ["Performance Metrics", "Cost Metrics", "Model Comparison"]
            )
            
            if st.button("Generate CSV Download"):
                if export_data == "Performance Metrics":
                    csv_data = data.get('performance_metrics', pd.DataFrame())
                elif export_data == "Cost Metrics":
                    csv_data = data.get('cost_metrics', pd.DataFrame())
                else:  # Model Comparison
                    csv_data = data.get('model_comparison', pd.DataFrame())
                
                if not csv_data.empty:
                    csv_string = csv_data.to_csv(index=False)
                    st.download_button(
                        label="Download CSV",
                        data=csv_string,
                        file_name=f"{export_data.lower().replace(' ', '_')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.error("No data available for export.")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 1rem;'>
        <p>LLM Evaluation Dashboard | Built with Streamlit & Plotly</p>
        <p>Data source: {}</p>
    </div>
    """.format(data_directory), unsafe_allow_html=True)

if __name__ == "__main__":
    main()