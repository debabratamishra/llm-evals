# llm-evals
A simple and efficient evaluation interface to compare performance between different large language models
